{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **Phrase 분석**\n",
    "\n",
    "<br>\n",
    "## **1 Phrase 구조객체 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('민병삼', 'Noun'), ('대령', 'Noun'), ('의', 'Josa'), ('항', 'Noun'), ('명', 'Suffix'), ('행위', 'Noun'), ('로', 'Josa'), ('초치', 'Noun'), ('하다', 'Verb')]\n",
      "CPU times: user 3.19 s, sys: 96.2 ms, total: 3.29 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = '민병삼 대령의 항명행위로 초치했다'\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "words = twitter.pos(text, stem=True)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.RegexpParser with 3 stages>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "grammar = \"\"\"\n",
    "NP: {<N.*>*<Suffix>?}   # 명사구를 정의한다\n",
    "VP: {<V.*>*}            # 동사구를 정의한다\n",
    "AP: {<A.*>*}            # 형용사구를 정의한다 \"\"\"\n",
    "parser = RegexpParser(grammar)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [('민병삼', 'Noun'), ('대령', 'Noun')]), ('의', 'Josa'), Tree('NP', [('항', 'Noun'), ('명', 'Suffix')]), Tree('NP', [('행위', 'Noun')]), ('로', 'Josa'), Tree('NP', [('초치', 'Noun')]), Tree('VP', [('하다', 'Verb')])])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = parser.parse(words)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('민병삼', 'Noun'), ('대령', 'Noun')],\n",
       " [('항', 'Noun'), ('명', 'Suffix')],\n",
       " [('행위', 'Noun')],\n",
       " [('초치', 'Noun')],\n",
       " [('하다', 'Verb')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tree = [list(txt)    for txt in chunks.subtrees()]\n",
    "text_tree[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **2 단어 Token을  활용한 통계적 문장분석**\n",
    "Zip'f Law\n",
    "\n",
    "<br>\n",
    "## **1 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독일 퀘르버 재단 연설문 : 베를린 선언\n",
    "f     = open('./data/베를린선언.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "texts_token = word_tokenize(texts_org)\n",
    "texts_token_dict = dict(FreqDist(texts_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".         168\n",
       ",          67\n",
       "것입니다       28\n",
       "한반도        20\n",
       "함께         18\n",
       "있습니다       16\n",
       "합니다        16\n",
       "수          15\n",
       "위한         13\n",
       "북한의        12\n",
       "우리         12\n",
       "나는         11\n",
       "북한이        11\n",
       "한반도의       11\n",
       "이          11\n",
       "여러분        10\n",
       "남과         10\n",
       "독일         10\n",
       "평화를        10\n",
       "있는          9\n",
       "‘           8\n",
       "남북          8\n",
       "’           8\n",
       "더           7\n",
       "내외          7\n",
       "귀빈          7\n",
       "그           6\n",
       "평화와         6\n",
       "국제사회의       6\n",
       "세계의         6\n",
       "         ... \n",
       "상황과         1\n",
       "정치‧군사적      1\n",
       "비정치적        1\n",
       "다섯째         1\n",
       "경제모델을       1\n",
       "그때          1\n",
       "하면          1\n",
       "실천하기만       1\n",
       "정상선언을       1\n",
       "공동번영할       1\n",
       "교량국가로       1\n",
       "해양을         1\n",
       "대륙과         1\n",
       "추진될         1\n",
       "철도는         1\n",
       "협력사업들도      1\n",
       "연결          1\n",
       "가스관         1\n",
       "남·북·러       1\n",
       "달릴          1\n",
       "유럽으로        1\n",
       "러시아와        1\n",
       "북경으로        1\n",
       "평양과         1\n",
       "열차가         1\n",
       "출발한         1\n",
       "목포에서        1\n",
       "부산과         1\n",
       "이어질         1\n",
       "유효합니다       1\n",
       "Length: 1186, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "texts_token_series = pd.Series(texts_token_dict)\n",
    "texts_token_series.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 불용어 처리**\n",
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like such a wonderful snow ice cream'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = 'I like such a Wonderful Snow Ice Cream'\n",
    "texts = texts.lower()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'such', 'a', 'wonderful', 'snow', 'ice', 'cream']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(texts)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'her', 'those', 'an', 'into', 'further', 'such', 'now', 'mightn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[::18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'wonderful', 'snow', 'ice', 'cream']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word   for word in tokens   \n",
    "                 if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'이': 'NP',\n",
       " '있': 'VA',\n",
       " '하': 'VV',\n",
       " '것': 'NNB',\n",
       " '들': 'VV',\n",
       " '그': 'MM',\n",
       " '되': 'VV',\n",
       " '수': 'NNB',\n",
       " '보': 'VX',\n",
       " '않': 'VX',\n",
       " '없': 'VA',\n",
       " '나': 'VX',\n",
       " '사람': 'NNG',\n",
       " '주': 'VV',\n",
       " '아니': 'VCN',\n",
       " '등': 'NNB',\n",
       " '같': 'VA',\n",
       " '우리': 'NP',\n",
       " '때': 'NNG',\n",
       " '년': 'NNB',\n",
       " '가': 'VV',\n",
       " '한': 'MM',\n",
       " '지': 'VX',\n",
       " '대하': 'VV',\n",
       " '오': 'VV',\n",
       " '말': 'VX',\n",
       " '일': 'NNB',\n",
       " '그렇': 'VA',\n",
       " '위하': 'VV',\n",
       " '때문': 'NNB',\n",
       " '그것': 'NP',\n",
       " '두': 'VV',\n",
       " '말하': 'VV',\n",
       " '알': 'VV',\n",
       " '그러나': 'MAJ',\n",
       " '받': 'VV',\n",
       " '못하': 'VX',\n",
       " '그런': 'MM',\n",
       " '또': 'MAG',\n",
       " '문제': 'NNG',\n",
       " '더': 'MAG',\n",
       " '사회': 'NNG',\n",
       " '많': 'VA',\n",
       " '그리고': 'MAJ',\n",
       " '좋': 'VA',\n",
       " '크': 'VA',\n",
       " '따르': 'VV',\n",
       " '중': 'NNB',\n",
       " '나오': 'VV',\n",
       " '가지': 'VV',\n",
       " '씨': 'NNB',\n",
       " '시키': 'XSV',\n",
       " '만들': 'VV',\n",
       " '지금': 'NNG',\n",
       " '생각하': 'VV',\n",
       " '그러': 'VV',\n",
       " '속': 'NNG',\n",
       " '하나': 'NR',\n",
       " '집': 'NNG',\n",
       " '살': 'VV',\n",
       " '모르': 'VV',\n",
       " '적': 'XSN',\n",
       " '월': 'NNB',\n",
       " '데': 'NNB',\n",
       " '자신': 'NNG',\n",
       " '안': 'MAG',\n",
       " '어떤': 'MM',\n",
       " '내': 'VV',\n",
       " '경우': 'NNG',\n",
       " '명': 'NNB',\n",
       " '생각': 'NNG',\n",
       " '시간': 'NNG',\n",
       " '그녀': 'NP',\n",
       " '다시': 'MAG',\n",
       " '이런': 'MM',\n",
       " '앞': 'NNG',\n",
       " '보이': 'VV',\n",
       " '번': 'NNB',\n",
       " '다른': 'MM',\n",
       " '어떻': 'VA',\n",
       " '여자': 'NNG',\n",
       " '개': 'NNB',\n",
       " '전': 'NNG',\n",
       " '사실': 'NNG',\n",
       " '이렇': 'VA',\n",
       " '점': 'NNG',\n",
       " '싶': 'VX',\n",
       " '정도': 'NNG',\n",
       " '좀': 'MAG',\n",
       " '원': 'NNB',\n",
       " '잘': 'MAG',\n",
       " '통하': 'VV',\n",
       " '소리': 'NNG',\n",
       " '놓': 'VX'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/한국어불용어100.txt', 'r')\n",
    "s = f.read(); f.close()\n",
    "\n",
    "stop_words = [ txt.split('\\t')[:2]  for txt in s.split('\\n') ]\n",
    "stopword   = {}\n",
    "for txt in stop_words:\n",
    "    try:    stopword[txt[0]] = txt[1]\n",
    "    except: pass\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **3 Token 객체를 활용한 통계적 분석**\n",
    "1. 레벤슈타인의 편집거리\n",
    "1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"자연 언어에 대한 연구는 오래전부터 이어져 오고 있음에도 2018년까지도 사람처럼 이해하지는 못한다.\".split()\n",
    "text2 = \"자연 언어에 대한 연구는 오래전부터 이어져 들어서도 아직 컴퓨터가 사람처럼 이해하지는 못한다.\".split()\n",
    "text3 = \"자연 아직 컴퓨터가 언어에 들어서도 못한다 이어져 사람처럼 이해하지는 대한 연구는 오래전부터.\".split()\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance('파이썬 알고리즘', '파파미 알탕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 : 3 \n",
      "어휘 순서를 바꿨을 때 : 10\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 : {} \\n어휘 순서를 바꿨을 때 : {}'.format(\n",
    "    edit_distance(text1, text2), \n",
    "    edit_distance(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 02 accuracy 정확도 측정\n",
    "from nltk.metrics import accuracy\n",
    "accuracy('파이썬', '파이프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.08333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    accuracy(text1, text2), \n",
    "    accuracy(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **4 Token 고유 객체를 활용한 통계적 검증방법**\n",
    "1. precision = Correct / (Correct + Incorrect + Missing)\n",
    "1. recall = Correct / (Correct + Incorrect + Spurious<가짜>)\n",
    "1. f_measure = (2 X Precision X Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = set(text1)\n",
    "text2 = set(text2)\n",
    "text3 = set(text3)\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import precision\n",
    "precision({'파이썬'}, {'파르썬'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    precision(set(text1), set(text2)), \n",
    "    precision(set(text2), set(text3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import recall\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    recall(text1, text2), \n",
    "    recall(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import f_measure\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    f_measure(text1, text2), \n",
    "    f_measure(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **3 N-gram 의 활용**\n",
    "N-Gram\n",
    "\n",
    "<br>\n",
    "## **1 N-gram 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는',\n",
       " '여러분',\n",
       " '고국에',\n",
       " '여러분',\n",
       " '하울젠',\n",
       " '쾨르버재단',\n",
       " '이사님과',\n",
       " '모드로',\n",
       " '총리님을',\n",
       " '비롯한',\n",
       " '여러분',\n",
       " '냉전과',\n",
       " '분단을',\n",
       " '통일을',\n",
       " '이루고',\n",
       " '힘으로',\n",
       " '유럽통합과',\n",
       " '국제평화를',\n",
       " '선도하고',\n",
       " '독일과']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_sample = texts_Berlin[:20]\n",
    "texts_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('존경하는', '여러분', '고국에'),\n",
       " ('여러분', '고국에', '여러분'),\n",
       " ('고국에', '여러분', '하울젠'),\n",
       " ('여러분', '하울젠', '쾨르버재단'),\n",
       " ('하울젠', '쾨르버재단', '이사님과')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "texts_sample = [txt for txt in ngrams(texts_sample, 3)]\n",
    "texts_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 Point wise Mutual Information**\n",
    "PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국에', '계신', '국민', '여러분', '하울젠', '쾨르버재단']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer('[가-힣]\\w+')\n",
    "raw_texts = re_capt.tokenize(texts_Berlin_raw)\n",
    "raw_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'존경하는 독일 국민 여러분 고국에 계신 국민 여러분 하울젠 쾨르버재단 이사님과 모드로 동독 총리님을 비롯한 내외 귀빈 여러분 먼저 냉전과 분단을 넘어 통일을 이루고 힘으로 유럽통합과 국제평화를 선도하고 있는 독일과 독일 국민에게 무한한 경의를 표합니다 오늘 자리를 마련해 주신 독일 정부와 쾨르버 재단에도 감사드립니다 아울러 얼마 별세하신 헬무트 총리의 가족'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ''\n",
    "for txt in raw_texts:\n",
    "    texts += txt + \" \"\n",
    "texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 52.3 ms, total: 1.82 s\n",
      "Wall time: 592 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 베를린 선언문에 Tag 속성 추가하기\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "tagged_words = twitter.pos(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2-1 Bi-Gram 을 대상으로 한 PMI**\n",
    "최상위 우도값 10개를 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7ff08681f588>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "finder = collocations.BigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun')),\n",
       " (('감사', 'Noun'), ('드립니', 'Verb')),\n",
       " (('갖춰', 'Verb'), ('지', 'PreEomi')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('거나', 'Eomi'), ('깨져', 'Verb')),\n",
       " (('건너지', 'Verb'), ('않기', 'Verb')),\n",
       " (('걷어', 'Verb'), ('차는', 'Verb')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.BigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2-2 Tri-Gram 을 대상으로한 PMI**\n",
    "최대우도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.TrigramCollocationFinder at 0x7ff08681fac8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = collocations.TrigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun'), ('역적', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun'), ('연결', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun'), ('생존', 'Noun')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun'), ('대', 'Suffix')),\n",
       " (('거나', 'Eomi'), ('깨져', 'Verb'), ('서도', 'Noun')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('견', 'Noun'), ('지하', 'Noun'), ('면서', 'Noun')),\n",
       " (('과도', 'Josa'), ('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('들어서는', 'Verb'), ('대전', 'Noun'), ('환', 'Noun')),\n",
       " (('록', 'Eomi'), ('앞장서서', 'Verb'), ('돕겠', 'Verb'))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.TrigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 Token List 객체를 활용하여\n",
    "# 해당 단어 주변에 위치하는 내용들을 살펴볼 수 있도록 돕는 메소드\n",
    "# ko = nltk.Text(tokens_ko, name='법안제출')\n",
    "# ko.concordence('초등학교')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **4 은닉 마르코프 모델 추정**\n",
    "Hidden Markov Model estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **1 HMM 알고리즘을 활용**\n",
    "문장분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는 독일 국민 여러분,',\n",
       " '고국에 계신 국민 여러분,',\n",
       " '하울젠 쾨르버재단 이사님과 모드로 전 동독 총리님을 비롯한 내외 귀빈 여러분,',\n",
       " '\\n먼저, 냉전과 분단을 넘어 통일을 이루고,',\n",
       " '그 힘으로 유럽통합과 국제평화를 선도하고 있는',\n",
       " '독일과 독일 국민에게 무한한 경의를 표합니다.',\n",
       " '\\n오늘 이 자리를 마련해 주신',\n",
       " '독일 정부와 쾨르버 재단에도 감사드립니다.',\n",
       " '\\n아울러, 얼마 전 별세하신 故 헬무트 콜 총리의 가족과 ',\n",
       " '독일 국민들에게 깊 은 애도와 위로의 마음을 전합니다.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = texts_Berlin_raw.split('\\n\\n')\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는 독일 국민 여러분',\n",
       " '고국에 계신 국민 여러분',\n",
       " '하울젠 쾨르버재단 이사님과 모드로 전 동독 총리님을 비롯한 내외 귀빈 여러분',\n",
       " '먼저 냉전과 분단을 넘어 통일을 이루고',\n",
       " '그 힘으로 유럽통합과 국제평화를 선도하고 있는',\n",
       " '독일과 독일 국민에게 무한한 경의를 표합니다',\n",
       " '오늘 이 자리를 마련해 주신',\n",
       " '독일 정부와 쾨르버 재단에도 감사드립니다',\n",
       " '아울러 얼마 전 별세하신 故 헬무트 콜 총리의 가족과 ',\n",
       " '독일 국민들에게 깊 은 애도와 위로의 마음을 전합니다']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [text.replace('\\n', '')  for text in texts]\n",
    "texts = [text.replace(',', '')  for text in texts]\n",
    "texts = [text.replace('.', '')  for text in texts]\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('존경하는', 'Verb'), ('독일', 'Noun'), ('국민', 'Noun'), ('여러분', 'Noun')],\n",
       " [('고국', 'Noun'),\n",
       "  ('에', 'Josa'),\n",
       "  ('계신', 'Verb'),\n",
       "  ('국민', 'Noun'),\n",
       "  ('여러분', 'Noun')]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words = [twitter.pos(text)  for text in texts]\n",
    "tagged_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Verb',\n",
       " 'Noun',\n",
       " 'Josa',\n",
       " 'Suffix',\n",
       " 'Adjective',\n",
       " 'Eomi',\n",
       " 'Exclamation',\n",
       " 'Foreign',\n",
       " 'Determiner',\n",
       " 'Number',\n",
       " 'Punctuation',\n",
       " 'Alpha',\n",
       " 'PreEomi',\n",
       " 'Adverb',\n",
       " 'Conjunction',\n",
       " 'VerbPrefix']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import unique_list\n",
    "tag_set = unique_list( tag     for sent        in  tagged_words     \n",
    "                               for (word, tag) in  sent )\n",
    "print(len(tag_set))\n",
    "tag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국', '에', '계신', '하울', '젠', '쾨르버']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = unique_list( word     for sent        in  tagged_words    \n",
    "                                for (word, tag) in  sent )\n",
    "print(len(symbols))\n",
    "symbols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.69%\n"
     ]
    }
   ],
   "source": [
    "# 히든 마르코프 훈련모듈 활성화 (16개 tag,  987개 문장) \n",
    "def train_and_test(tagged_words, est):\n",
    "    from nltk.util import unique_list\n",
    "    tag_set = unique_list( tag   for sent        in  tagged_words     \n",
    "                                 for (word, tag) in  sent )\n",
    "    symbols = unique_list( word  for sent        in  tagged_words    \n",
    "                                 for (word, tag) in  sent )\n",
    "    import nltk\n",
    "    trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "    train_corpus, test_corpus = [], []  \n",
    "    for i in range(len(tagged_words)) :\n",
    "        if i % 10 :  \n",
    "            train_corpus += [tagged_words[i]]\n",
    "        else :       \n",
    "            test_corpus += [tagged_words[i]]\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator = est)\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "train_and_test(tagged_words, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 HMM 영문분석**\n",
    "Brown 어휘목록을 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag set  : 92 \n",
      "Word set : 1464\n",
      "22.75%\n"
     ]
    }
   ],
   "source": [
    "def HMM_english():\n",
    "    import nltk\n",
    "    cor = nltk.corpus.brown.tagged_sents(categories='adventure')[:500]\n",
    "\n",
    "    from nltk.util import unique_list\n",
    "    tag_set = unique_list( tag  for sent        in  cor     \n",
    "                                for (word, tag) in  sent )\n",
    "    symbols = unique_list( word  for sent        in  cor    \n",
    "                                 for (word, tag) in  sent )\n",
    "    print(\"tag set  : {} \\nWord set : {}\".format(len(tag_set), len(symbols)))\n",
    "\n",
    "    train_corpus, test_corpus = [], []  \n",
    "    trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "    for i in range(len(cor)) :\n",
    "        if i % 10 :  train_corpus += [cor[i]]   # train 90% , test 10% 데이터 생성\n",
    "        else :       test_corpus += [cor[i]]\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=None)  # .train_supervised()\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "HMM_english()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **5 TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **1 영문 데이터 전처리**\n",
    "트럼프 취임사 연설문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief', 'Justice', 'Roberts', ',', 'President']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/trump.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "texts = word_tokenize(texts_org)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_eng = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "punct = string.punctuation\n",
    "punct = [punct[i] for i in range(len(punct))]\n",
    "punct = punct + stopword_eng + ['\\n'] \n",
    "len(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [txt.lower()    for txt in texts   if txt.lower() not in punct]\n",
    "document = ''\n",
    "for txt in texts:\n",
    "    document += txt + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 tf idf **\n",
    "연설문내 단어들의 빈도를 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x456 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 456 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec   = TfidfVectorizer()\n",
    "transformed = tfidf_vec.fit_transform(raw_documents = [document])\n",
    "index_value = {i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_indexed = []\n",
    "\n",
    "import numpy as np\n",
    "transformed = np.array(transformed.todense())\n",
    "\n",
    "for row in transformed:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in enumerate(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "america     0.423619\n",
       "american    0.232990\n",
       "people      0.211809\n",
       "country     0.190628\n",
       "nation      0.190628\n",
       "one         0.169447\n",
       "every       0.148266\n",
       "never       0.127086\n",
       "world       0.127086\n",
       "back        0.127086\n",
       "new         0.127086\n",
       "great       0.127086\n",
       "make        0.105905\n",
       "god         0.105905\n",
       "today       0.105905\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tfidf = pd.Series(fully_indexed[0]).sort_values(ascending=False)\n",
    "tfidf[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12708556268223822"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['great']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
