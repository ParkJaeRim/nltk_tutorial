{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **Phrase 분석**\n",
    "\n",
    "<br>\n",
    "## **1 불용어 처리**\n",
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like such a wonderful snow ice cream'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = 'I like such a Wonderful Snow Ice Cream'\n",
    "texts = texts.lower()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'such', 'a', 'wonderful', 'snow', 'ice', 'cream']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(texts)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'her', 'those', 'an', 'into', 'further', 'such', 'now', 'mightn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[::18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'wonderful', 'snow', 'ice', 'cream']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word   for word in tokens   \n",
    "                 if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 한글의 불용어 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'이': 'NP',\n",
       " '있': 'VA',\n",
       " '하': 'VV',\n",
       " '것': 'NNB',\n",
       " '들': 'VV',\n",
       " '그': 'MM',\n",
       " '되': 'VV',\n",
       " '수': 'NNB',\n",
       " '보': 'VX',\n",
       " '않': 'VX',\n",
       " '없': 'VA',\n",
       " '나': 'VX',\n",
       " '사람': 'NNG',\n",
       " '주': 'VV',\n",
       " '아니': 'VCN',\n",
       " '등': 'NNB',\n",
       " '같': 'VA',\n",
       " '우리': 'NP',\n",
       " '때': 'NNG',\n",
       " '년': 'NNB',\n",
       " '가': 'VV',\n",
       " '한': 'MM',\n",
       " '지': 'VX',\n",
       " '대하': 'VV',\n",
       " '오': 'VV',\n",
       " '말': 'VX',\n",
       " '일': 'NNB',\n",
       " '그렇': 'VA',\n",
       " '위하': 'VV',\n",
       " '때문': 'NNB',\n",
       " '그것': 'NP',\n",
       " '두': 'VV',\n",
       " '말하': 'VV',\n",
       " '알': 'VV',\n",
       " '그러나': 'MAJ',\n",
       " '받': 'VV',\n",
       " '못하': 'VX',\n",
       " '그런': 'MM',\n",
       " '또': 'MAG',\n",
       " '문제': 'NNG',\n",
       " '더': 'MAG',\n",
       " '사회': 'NNG',\n",
       " '많': 'VA',\n",
       " '그리고': 'MAJ',\n",
       " '좋': 'VA',\n",
       " '크': 'VA',\n",
       " '따르': 'VV',\n",
       " '중': 'NNB',\n",
       " '나오': 'VV',\n",
       " '가지': 'VV',\n",
       " '씨': 'NNB',\n",
       " '시키': 'XSV',\n",
       " '만들': 'VV',\n",
       " '지금': 'NNG',\n",
       " '생각하': 'VV',\n",
       " '그러': 'VV',\n",
       " '속': 'NNG',\n",
       " '하나': 'NR',\n",
       " '집': 'NNG',\n",
       " '살': 'VV',\n",
       " '모르': 'VV',\n",
       " '적': 'XSN',\n",
       " '월': 'NNB',\n",
       " '데': 'NNB',\n",
       " '자신': 'NNG',\n",
       " '안': 'MAG',\n",
       " '어떤': 'MM',\n",
       " '내': 'VV',\n",
       " '경우': 'NNG',\n",
       " '명': 'NNB',\n",
       " '생각': 'NNG',\n",
       " '시간': 'NNG',\n",
       " '그녀': 'NP',\n",
       " '다시': 'MAG',\n",
       " '이런': 'MM',\n",
       " '앞': 'NNG',\n",
       " '보이': 'VV',\n",
       " '번': 'NNB',\n",
       " '다른': 'MM',\n",
       " '어떻': 'VA',\n",
       " '여자': 'NNG',\n",
       " '개': 'NNB',\n",
       " '전': 'NNG',\n",
       " '사실': 'NNG',\n",
       " '이렇': 'VA',\n",
       " '점': 'NNG',\n",
       " '싶': 'VX',\n",
       " '정도': 'NNG',\n",
       " '좀': 'MAG',\n",
       " '원': 'NNB',\n",
       " '잘': 'MAG',\n",
       " '통하': 'VV',\n",
       " '소리': 'NNG',\n",
       " '놓': 'VX'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/한국어불용어100.txt', 'r')\n",
    "s = f.read(); f.close()\n",
    "\n",
    "stop_words = [ txt.split('\\t')[:2]  for txt in s.split('\\n') ]\n",
    "stopword   = {}\n",
    "for txt in stop_words:\n",
    "    try:    stopword[txt[0]] = txt[1]\n",
    "    except: pass\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **3 Token 객체를 활용한 통계적 분석**\n",
    "1. 레벤슈타인의 편집거리\n",
    "1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"자연 언어에 대한 연구는 오래전부터 이어져 오고 있음에도 2018년까지도 사람처럼 이해하지는 못한다.\".split()\n",
    "text2 = \"자연 언어에 대한 연구는 오래전부터 이어져 들어서도 아직 컴퓨터가 사람처럼 이해하지는 못한다.\".split()\n",
    "text3 = \"자연 아직 컴퓨터가 언어에 들어서도 못한다 이어져 사람처럼 이해하지는 대한 연구는 오래전부터.\".split()\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance('파이썬 알고리즘', '파파미 알탕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 : 3 \n",
      "어휘 순서를 바꿨을 때 : 10\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 : {} \\n어휘 순서를 바꿨을 때 : {}'.format(\n",
    "    edit_distance(text1, text2), \n",
    "    edit_distance(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 02 accuracy 정확도 측정\n",
    "from nltk.metrics import accuracy\n",
    "accuracy('파이썬', '파이프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.08333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    accuracy(text1, text2), \n",
    "    accuracy(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **4 Token 고유 객체를 활용한 통계적 검증방법**\n",
    "1. precision = Correct / (Correct + Incorrect + Missing)\n",
    "1. recall = Correct / (Correct + Incorrect + Spurious<가짜>)\n",
    "1. f_measure = (2 X Precision X Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = set(text1)\n",
    "text2 = set(text2)\n",
    "text3 = set(text3)\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import precision\n",
    "precision({'파이썬'}, {'파르썬'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    precision(set(text1), set(text2)), \n",
    "    precision(set(text2), set(text3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import recall\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    recall(text1, text2), \n",
    "    recall(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import f_measure\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    f_measure(text1, text2), \n",
    "    f_measure(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **3 N-gram 의 활용**\n",
    "<br>\n",
    "## **1 N-gram 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독일 퀘르버 재단 연설문 : 베를린 선언\n",
    "f     = open('./data/베를린선언.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_token = word_tokenize(texts_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('존경하는', '독일', '국민'),\n",
       " ('독일', '국민', '여러분'),\n",
       " ('국민', '여러분', ','),\n",
       " ('여러분', ',', '고국에'),\n",
       " (',', '고국에', '계신')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "texts_sample = [txt for txt in ngrams(word_token, 3)]\n",
    "texts_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 Point wise Mutual Information**\n",
    "PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국에', '계신', '국민', '여러분', '하울젠', '쾨르버재단']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer('[가-힣]\\w+')\n",
    "raw_texts = re_capt.tokenize(texts_org)\n",
    "raw_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'존경하는 독일 국민 여러분 고국에 계신 국민 여러분 하울젠 쾨르버재단 이사님과 모드로 동독 총리님을 비롯한 내외 귀빈 여러분 먼저 냉전과 분단을 넘어 통일을 이루고 힘으로 유럽통합과 국제평화를 선도하고 있는 독일과 독일 국민에게 무한한 경의를 표합니다 오늘 자리를 마련해 주신 독일 정부와 쾨르버 재단에도 감사드립니다 아울러 얼마 별세하신 헬무트 총리의 가족'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ''\n",
    "for txt in raw_texts:\n",
    "    texts += txt + \" \"\n",
    "texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 93 ms, total: 4.69 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 베를린 선언문에 Tag 속성 추가하기\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "tagged_words = twitter.pos(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2-1 Bi-Gram 을 대상으로 한 PMI**\n",
    "최상위 우도값 10개를 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7f6c7e3064a8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "finder = collocations.BigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun')),\n",
       " (('감사', 'Noun'), ('드립니', 'Verb')),\n",
       " (('갖춰', 'Verb'), ('지', 'PreEomi')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('거나', 'Eomi'), ('깨져', 'Verb')),\n",
       " (('건너지', 'Verb'), ('않기', 'Verb')),\n",
       " (('걷어', 'Verb'), ('차는', 'Verb')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.BigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2-2 Tri-Gram 을 대상으로한 PMI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.TrigramCollocationFinder at 0x7f6c6af56f60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = collocations.TrigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun'), ('역적', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun'), ('연결', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun'), ('생존', 'Noun')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun'), ('대', 'Suffix')),\n",
       " (('거나', 'Eomi'), ('깨져', 'Verb'), ('서도', 'Noun')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('견', 'Noun'), ('지하', 'Noun'), ('면서', 'Noun')),\n",
       " (('과도', 'Josa'), ('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('들어서는', 'Verb'), ('대전', 'Noun'), ('환', 'Noun')),\n",
       " (('록', 'Eomi'), ('앞장서서', 'Verb'), ('돕겠', 'Verb'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.TrigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **4 은닉 마르코프 모델 추정**\n",
    "Hidden Markov Model estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **1 HMM 알고리즘을 활용**\n",
    "문장분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는 독일 국민 여러분,',\n",
       " '고국에 계신 국민 여러분,',\n",
       " '하울젠 쾨르버재단 이사님과 모드로 전 동독 총리님을 비롯한 내외 귀빈 여러분,',\n",
       " '\\n먼저, 냉전과 분단을 넘어 통일을 이루고,',\n",
       " '그 힘으로 유럽통합과 국제평화를 선도하고 있는',\n",
       " '독일과 독일 국민에게 무한한 경의를 표합니다.',\n",
       " '\\n오늘 이 자리를 마련해 주신',\n",
       " '독일 정부와 쾨르버 재단에도 감사드립니다.',\n",
       " '\\n아울러, 얼마 전 별세하신 故 헬무트 콜 총리의 가족과 ',\n",
       " '독일 국민들에게 깊 은 애도와 위로의 마음을 전합니다.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = texts_org.split('\\n\\n')\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는 독일 국민 여러분',\n",
       " '고국에 계신 국민 여러분',\n",
       " '하울젠 쾨르버재단 이사님과 모드로 전 동독 총리님을 비롯한 내외 귀빈 여러분',\n",
       " '먼저 냉전과 분단을 넘어 통일을 이루고',\n",
       " '그 힘으로 유럽통합과 국제평화를 선도하고 있는',\n",
       " '독일과 독일 국민에게 무한한 경의를 표합니다',\n",
       " '오늘 이 자리를 마련해 주신',\n",
       " '독일 정부와 쾨르버 재단에도 감사드립니다',\n",
       " '아울러 얼마 전 별세하신 故 헬무트 콜 총리의 가족과 ',\n",
       " '독일 국민들에게 깊 은 애도와 위로의 마음을 전합니다']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [text.replace('\\n', '')  for text in texts]\n",
    "texts = [text.replace(',', '')  for text in texts]\n",
    "texts = [text.replace('.', '')  for text in texts]\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('존경하는', 'Verb'), ('독일', 'Noun'), ('국민', 'Noun'), ('여러분', 'Noun')],\n",
       " [('고국', 'Noun'),\n",
       "  ('에', 'Josa'),\n",
       "  ('계신', 'Verb'),\n",
       "  ('국민', 'Noun'),\n",
       "  ('여러분', 'Noun')]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words = [twitter.pos(text)  for text in texts]\n",
    "tagged_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Verb',\n",
       " 'Noun',\n",
       " 'Josa',\n",
       " 'Suffix',\n",
       " 'Adjective',\n",
       " 'Eomi',\n",
       " 'Exclamation',\n",
       " 'Foreign',\n",
       " 'Determiner',\n",
       " 'Number',\n",
       " 'Punctuation',\n",
       " 'Alpha',\n",
       " 'PreEomi',\n",
       " 'Adverb',\n",
       " 'Conjunction',\n",
       " 'VerbPrefix']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import unique_list\n",
    "tag_set = unique_list( tag     for sent        in  tagged_words     \n",
    "                               for (word, tag) in  sent )\n",
    "print(len(tag_set))\n",
    "tag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국', '에', '계신', '하울', '젠', '쾨르버']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = unique_list( word     for sent        in  tagged_words    \n",
    "                                for (word, tag) in  sent )\n",
    "print(len(symbols))\n",
    "symbols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.69%\n",
      "CPU times: user 196 ms, sys: 0 ns, total: 196 ms\n",
      "Wall time: 70.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 히든 마르코프 훈련모듈 활성화 (16개 tag,  987개 문장) \n",
    "def train_and_test(tagged_words, est):\n",
    "    from nltk.util import unique_list\n",
    "    tag_set = unique_list( tag   for sent        in  tagged_words     \n",
    "                                 for (word, tag) in  sent )\n",
    "    symbols = unique_list( word  for sent        in  tagged_words    \n",
    "                                 for (word, tag) in  sent )\n",
    "    import nltk\n",
    "    trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "    train_corpus, test_corpus = [], []  \n",
    "    for i in range(len(tagged_words)) :\n",
    "        if i % 10 :  \n",
    "            train_corpus += [tagged_words[i]]\n",
    "        else :       \n",
    "            test_corpus += [tagged_words[i]]\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator = est)\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "train_and_test(tagged_words, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 HMM 영문분석**\n",
    "Brown 어휘목록을 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag set  : 92 \n",
      "Word set : 1464\n",
      "22.75%\n",
      "CPU times: user 573 ms, sys: 15.7 ms, total: 589 ms\n",
      "Wall time: 507 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def HMM_english():\n",
    "    import nltk\n",
    "    cor = nltk.corpus.brown.tagged_sents(categories='adventure')[:500]\n",
    "\n",
    "    from nltk.util import unique_list\n",
    "    tag_set = unique_list( tag  for sent        in  cor     \n",
    "                                for (word, tag) in  sent )\n",
    "    symbols = unique_list( word  for sent        in  cor    \n",
    "                                 for (word, tag) in  sent )\n",
    "    print(\"tag set  : {} \\nWord set : {}\".format(len(tag_set), len(symbols)))\n",
    "\n",
    "    train_corpus, test_corpus = [], []  \n",
    "    trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "    for i in range(len(cor)) :\n",
    "        if i % 10 :  train_corpus += [cor[i]]   # train 90% , test 10% 데이터 생성\n",
    "        else :       test_corpus += [cor[i]]\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=None)  # .train_supervised()\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "HMM_english()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **5 TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **1 영문 데이터 전처리**\n",
    "트럼프 취임사 연설문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief', 'Justice', 'Roberts', ',', 'President']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./data/trump.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "texts = word_tokenize(texts_org)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_eng = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "punct = string.punctuation\n",
    "punct = [punct[i] for i in range(len(punct))]\n",
    "punct = punct + stopword_eng + ['\\n'] \n",
    "len(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [txt.lower()    for txt in texts   if txt.lower() not in punct]\n",
    "document = ''\n",
    "for txt in texts:\n",
    "    document += txt + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 tf idf **\n",
    "연설문내 단어들의 빈도를 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.26 ms, sys: 11 µs, total: 4.27 ms\n",
      "Wall time: 4.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec   = TfidfVectorizer()\n",
    "transformed = tfidf_vec.fit_transform(raw_documents = [document])\n",
    "index_value = {i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_indexed = []\n",
    "\n",
    "import numpy as np\n",
    "transformed = np.array(transformed.todense())\n",
    "\n",
    "for row in transformed:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in enumerate(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "america     0.423619\n",
       "american    0.232990\n",
       "people      0.211809\n",
       "country     0.190628\n",
       "nation      0.190628\n",
       "one         0.169447\n",
       "every       0.148266\n",
       "never       0.127086\n",
       "world       0.127086\n",
       "back        0.127086\n",
       "new         0.127086\n",
       "great       0.127086\n",
       "make        0.105905\n",
       "god         0.105905\n",
       "today       0.105905\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tfidf = pd.Series(fully_indexed[0]).sort_values(ascending=False)\n",
    "tfidf[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12708556268223822"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['great']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
