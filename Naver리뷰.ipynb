{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **Naver 리뷰분석**\n",
    "by Gensim [Blog](https://medium.com/@hoho0443/konlpy-nltk-gensim%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%98%EC%97%AC-%EB%AC%B8%EC%9E%A5-%EA%B8%8D%EC%A0%95-%EB%B6%80%EC%A0%95-%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0-6e58ca9203cc) [박은정 PPT](https://www.lucypark.kr/docs/2015-pyconkr/#60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **1 Nltk_run.py**\n",
    "nltk 모듈을 활용하여 베이지안 모델을 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한글', '형태소', '분', '석기', '테스트', '중', '입니', '다']\n",
      "['한글', '형태소', '석기', '테스트', '중']\n",
      "[('한글', 'Noun'), ('형태소', 'Noun'), ('분', 'Suffix'), ('석기', 'Noun'), ('테스트', 'Noun'), ('중', 'Noun'), ('입니', 'Adjective'), ('다', 'Eomi'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "import nltk\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "print(twitter.morphs(u'한글형태소분석기 테스트 중 입니다')) # ??\n",
    "print(twitter.nouns(u'한글형태소분석기 테스트 중 입니다!')) #명사\n",
    "print(twitter.pos(u'한글형태소분석기 테스트 중 입니다.')) #형태소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터중 1/50 의 데이터를 대상으로 학습한다\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        \n",
    "    from random import randint\n",
    "    random_data = [data[randint(1, len(data))]  for no in range(int(len(data)/50)) ]\n",
    "    return random_data\n",
    "\n",
    "def tokenize(doc):\n",
    "  # norm, stem은 optional\n",
    "  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "def term_exists(doc):\n",
    "    return {'exists({})'.format(word): (word in set(doc)) for word in selected_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3\n",
      "1000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 트래이닝 데이터와 테스트 데이터를 읽기\n",
    "train_data = read_data('data/ratings_train.txt')\n",
    "test_data  = read_data('data/ratings_test.txt')\n",
    "\n",
    "# row, column의 수가 제대로 읽혔는지 확인\n",
    "print(len(train_data))      # nrows: 150000\n",
    "print(len(train_data[0]))   # ncols: 3\n",
    "print(len(test_data))       # nrows: 50000\n",
    "print(len(test_data[0]))     # ncols: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44795\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분류\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:]]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_data[1:]]\n",
    "#Training data의 token 모으기\n",
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./Punctuation', 1347), ('하다/Verb', 1052), ('영화/Noun', 1030), ('이/Josa', 806), ('보다/Verb', 716), ('../Punctuation', 616), ('의/Josa', 605), ('가/Josa', 553), ('에/Josa', 549), ('도/Josa', 482)]\n"
     ]
    }
   ],
   "source": [
    "# Load tokens with nltk.Text()\n",
    "text = nltk.Text(tokens, name='NMSC')\n",
    "print(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8078078078078078\n",
      "CPU times: user 18.7 s, sys: 304 ms, total: 19 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 텍스트간의 연어 빈번하게 등장하는 단어 구하기\n",
    "# text.collocations()\n",
    "# term이 존재하는지에 따라서 문서를 분류\n",
    "selected_words = [f[0] for f in text.vocab().most_common(2000)] # 여기서는 최빈도 단어 2000개를 피쳐로 사용\n",
    "train_docs     = train_docs[:10000] # 시간 단축을 위한 꼼수로 training corpus의 일부만 사용할 수 있음\n",
    "train_xy       = [(term_exists(d), c) for d, c in train_docs]\n",
    "test_xy        = [(term_exists(d), c) for d, c in test_docs]\n",
    "# nltk의 NaiveBayesClassifier으로 데이터를 트래이닝 시키고, test 데이터로 확인\n",
    "classifier     = nltk.NaiveBayesClassifier.train(train_xy) #Naive Bayes classifier 적용\n",
    "print(nltk.classify.accuracy(classifier, test_xy))\n",
    "# => 0.80418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         exists(최악/Noun) = True                0 : 1      =     28.1 : 1.0\n",
      "        exists(쓰레기/Noun) = True                0 : 1      =     16.9 : 1.0\n",
      "         exists(실망/Noun) = True                0 : 1      =     16.2 : 1.0\n",
      "          exists(냐/Josa) = True                0 : 1      =     12.1 : 1.0\n",
      "        exists(0/Number) = True                0 : 1      =     11.6 : 1.0\n",
      "         exists(알바/Noun) = True                0 : 1      =     11.6 : 1.0\n",
      "         exists(짜증/Noun) = True                0 : 1      =     11.3 : 1.0\n",
      "  exists(재미없다/Adjective) = True                0 : 1      =     10.6 : 1.0\n",
      "         exists(최고/Noun) = True                1 : 0      =     10.1 : 1.0\n",
      "         exists(보지/Noun) = True                0 : 1      =      9.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)\n",
    "#nltk.polarity_scores(\"i love you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 doc2Vec_train**\n",
    "nltk 모듈을 활용하여 베이지안 모델을 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from konlpy.tag import Twitter\n",
    "import multiprocessing\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "    from random import randint\n",
    "    random_data = [data[randint(1, len(data))]  for no in range(int(len(data)/50)) ]\n",
    "    return random_data\n",
    "\n",
    "def tokenize(doc):\n",
    "  # norm, stem은 optional\n",
    "  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec parameters\n",
    "cores = multiprocessing.cpu_count()\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "word_min_count = 2\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 1\n",
    "worker_count = cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트래이닝 데이터 읽기\n",
    "train_data = read_data('data/ratings_train.txt')\n",
    "# 형태소 분류\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:]]\n",
    "# doc2vec 에서 필요한 데이터 형식으로 변경\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# 사전 구축\n",
    "doc_vectorizer = doc2vec.Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(tagged_train_docs)\n",
    "\n",
    "# Train document vectors!\n",
    "for epoch in range(10):\n",
    "    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "    doc_vectorizer.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('중요하다/Adjective', 0.7721543312072754),\n",
      " ('판타지/Noun', 0.7557740211486816),\n",
      " ('성/Noun', 0.7301464080810547),\n",
      " ('에겐/Josa', 0.7294729351997375),\n",
      " ('코미디/Noun', 0.7204558849334717),\n",
      " ('주제/Noun', 0.7145422101020813),\n",
      " ('설득/Noun', 0.7118157744407654),\n",
      " ('공포영화/Noun', 0.7100128531455994),\n",
      " ('다큐/Noun', 0.7097640037536621),\n",
      " ('섹스/Noun', 0.7035242915153503)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "#To save\n",
    "doc_vectorizer.save('data/doc2vec.model')\n",
    "pprint(doc_vectorizer.wv.most_similar('공포/Noun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044130173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/python/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "pprint(doc_vectorizer.wv.similarity('공포/Noun', 'ㅋㅋ/KoreanParticle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **3 doc2Vec_Test**\n",
    "nltk 모듈을 활용하여 베 모델을 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from konlpy.tag import Twitter\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "    from random import randint\n",
    "    random_data = [data[randint(1, len(data))]  for no in range(int(len(data)/50)) ]\n",
    "    return random_data\n",
    "\n",
    "def tokenize(doc):\n",
    "  # norm, stem은 optional\n",
    "  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 읽기\n",
    "train_data = read_data('data/ratings_train.txt')\n",
    "test_data = read_data('data/ratings_test.txt')\n",
    "# 형태소 분류\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:]]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_data[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec 에서 필요한 데이터 형식으로 변경\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]\n",
    "tagged_test_docs = [TaggedDocument(d, [c]) for d, c in test_docs]\n",
    "# load train data\n",
    "doc_vectorizer = Doc2Vec.load('data/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류를 위한 피쳐 생성\n",
    "train_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]\n",
    "train_y = [doc.tags[0] for doc in tagged_train_docs]\n",
    "test_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]\n",
    "test_y = [doc.tags[0] for doc in tagged_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x, train_y)\n",
    "# 테스트 socre 확인\n",
    "print( classifier.score(test_x, test_y) )\n",
    "# 0.63904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'data/finalized_model.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **4 doc2Vec_run**\n",
    "nltk 모듈을 활용하여 베이지안 모델을 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from konlpy.tag import Twitter\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "    return data\n",
    "\n",
    "def tokenize(doc):\n",
    "  # norm, stem은 optional\n",
    "  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 구동 데이터를 읽기\n",
    "run_data = read_data('data/ratings_run.txt')\n",
    "# 형태소 분류\n",
    "run_docs = [(tokenize(row[1]), row[2]) for row in run_data[1:]]\n",
    "# doc2vec 에서 필요한 데이터 형식으로 변경\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "tagged_run_docs = [TaggedDocument(d, [c]) for d, c in run_docs]\n",
    "# load train data\n",
    "doc_vectorizer = Doc2Vec.load('data/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류를 위한 피쳐 생성\n",
    "run_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_run_docs]\n",
    "run_y = [doc.tags[0] for doc in tagged_run_docs]\n",
    "# load the model from disk\n",
    "filename = 'data/finalized_model.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n",
      "['1']\n"
     ]
    }
   ],
   "source": [
    "# 실제 분류 확인\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(loaded_model.predict(run_x[0].reshape(1, -1)))\n",
    "print(loaded_model.predict(run_x[1].reshape(1, -1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
