{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **Token의 개념**\n",
    "\n",
    "<br></br>\n",
    "## **1 Token**\n",
    "어휘분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"힘든 월요일, 아침.\n",
    "힘든 직장인 분들에게. 월요일 식혜를 제공합니다\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['힘든 월요일, 아침.', '힘든 직장인 분들에게.', '월요일 식혜를 제공합니다']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['힘든', '월요일', ',', '아침', '.', '힘든', '직장인', '분들에게', '.', '월요일', '식혜를', '제공합니다']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "text = word_tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 Token 의 빈도분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'힘든': 2,\n",
       " '월요일': 2,\n",
       " ',': 1,\n",
       " '아침': 1,\n",
       " '.': 2,\n",
       " '직장인': 1,\n",
       " '분들에게': 1,\n",
       " '식혜를': 1,\n",
       " '제공합니다': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "dict(FreqDist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 168, ',': 67, '것입니다': 28, '한반도': 20, '함께': 18, '합니다': 16, '있습니다': 16, '수': 15, '위한': 13, '우리': 12, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f         = open('../data/베를린선언.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()\n",
    "\n",
    "texts_token = word_tokenize(texts_org)\n",
    "FreqDist(texts_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_token_dict = dict(FreqDist(texts_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".         168\n",
       ",          67\n",
       "것입니다       28\n",
       "한반도        20\n",
       "함께         18\n",
       "있습니다       16\n",
       "합니다        16\n",
       "수          15\n",
       "위한         13\n",
       "북한의        12\n",
       "우리         12\n",
       "나는         11\n",
       "북한이        11\n",
       "한반도의       11\n",
       "이          11\n",
       "여러분        10\n",
       "남과         10\n",
       "독일         10\n",
       "평화를        10\n",
       "있는          9\n",
       "‘           8\n",
       "남북          8\n",
       "’           8\n",
       "더           7\n",
       "내외          7\n",
       "귀빈          7\n",
       "그           6\n",
       "평화와         6\n",
       "국제사회의       6\n",
       "세계의         6\n",
       "         ... \n",
       "상황과         1\n",
       "정치‧군사적      1\n",
       "비정치적        1\n",
       "다섯째         1\n",
       "경제모델을       1\n",
       "그때          1\n",
       "하면          1\n",
       "실천하기만       1\n",
       "정상선언을       1\n",
       "공동번영할       1\n",
       "교량국가로       1\n",
       "해양을         1\n",
       "대륙과         1\n",
       "추진될         1\n",
       "철도는         1\n",
       "협력사업들도      1\n",
       "연결          1\n",
       "가스관         1\n",
       "남·북·러       1\n",
       "달릴          1\n",
       "유럽으로        1\n",
       "러시아와        1\n",
       "북경으로        1\n",
       "평양과         1\n",
       "열차가         1\n",
       "출발한         1\n",
       "목포에서        1\n",
       "부산과         1\n",
       "이어질         1\n",
       "유효합니다       1\n",
       "Length: 1186, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! pip3 install pandas\n",
    "import pandas as pd\n",
    "texts_token_series = pd.Series(texts_token_dict)\n",
    "texts_token_series.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **3 Re 를 사용한 Regex 정규식**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010',\n",
       " '1234',\n",
       " '1234',\n",
       " '010',\n",
       " '8888',\n",
       " '9999',\n",
       " '010',\n",
       " '2123',\n",
       " '1299',\n",
       " '010',\n",
       " '222',\n",
       " '9999',\n",
       " '010',\n",
       " '555',\n",
       " '2345']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Park 010-1234-1234 Kim 010-8888-9999 \n",
    "Lee 010-2123-1299 한남충 010-222-9999 메갈녀 010-555-2345\"\"\"\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer(r'\\d+')\n",
    "re_capt.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Park', 'Kim', 'Lee']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer(r'[A-z]\\w+')\n",
    "re_capt.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한남충', '메갈녀']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer(r'[가-힣]\\w+')\n",
    "re_capt.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글과 영문 함께 이름만 추출기 [A-z가-힣]\n",
    "# 아이디어 1 : 숫자가 아닌 내용만 추출한다\n",
    "# 아이디어 2 : 한글과 영어만 추출한다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
