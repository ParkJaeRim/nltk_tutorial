{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sklearn 을 활용한 Tf-idf 활용**\n",
    "1. 벡터로 데이터 변환 활용 및 문서간 비교를 연구해보자\n",
    "1. https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n",
    "1. https://lovit.github.io/nlp/2018/03/26/from_text_to_matrix/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **1 DictVectorizer**\n",
    "feature_extraction 서브 패키지로, {dict} 정보를 받아 BOW 인코딩벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0.],\n",
       "       [0., 3., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v : 단어벡터로\n",
    "# A, B, C 문자를 숫자로 치환\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A': 1, 'B': 2}, {'B': 3, 'C': 1}]\n",
    "X = v.fit_transform(D)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 존재하는 단어벡터 내부에서만 추가됨\n",
    "# 따라서 D는 포함이 되지 않는다\n",
    "v.transform({'C': 4, 'D': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **CountVectorizer**\n",
    "CountVectorizer는 다음과 같은 세가지 작업을 수행한다.\n",
    "1. **문서를 토큰 리스트**로 변환한다.\n",
    "1. 각 문서에서 **토큰의 출현 빈도**를 센다.\n",
    "1. 각 문서를 **BOW 인코딩 벡터**로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "# 단어벡터 파라미터 내용 보기\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 생성한 단어벡터를 활용하여 문장분석\n",
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어벡터에 없는 단어들의 문장분석\n",
    "vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer 내부 메소드 객체\n",
    "1. **stop_words** : 문자열 {‘english’}, 리스트 또는 None (디폴트) stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "1. **analyzer** : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수 단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "1. **token_pattern** : string 토큰 정의용 정규 표현식\n",
    "1. **tokenizer** : 함수 또는 None (디폴트)토큰 생성 함수\n",
    "1. **ngram_range** : (min_n, max_n) 튜플 n-그램 범위\n",
    "1. **max_df** : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1 단어장에 포함되기 위한 최대 빈도\n",
    "1. **min_df** : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1 단어장에 포함되기 위한 최소 빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **StopWord**\n",
    "단어들의 대소문자 구분없이 자동으로 변환후 작업을 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus 문자열에서 stop_wors 를 제외한 객체를 출력\n",
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus 문자열에서 stop_wors 'english'를 제외한 객체를 출력\n",
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **Token**\n",
    "1. 단어들의 대소문자 구분없이 자동으로 변환후 작업을 한다\n",
    "1. **analyzer, tokenizer, token_pattern** 등의 인수로 사용할 토큰 생성기를 선택가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규식을 활용한 객체분류\n",
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 5,\n",
       " 'the': 9,\n",
       " 'first': 4,\n",
       " 'document': 3,\n",
       " '.': 0,\n",
       " 'second': 8,\n",
       " 'and': 2,\n",
       " 'third': 10,\n",
       " 'one': 7,\n",
       " '?': 1,\n",
       " 'last': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlkt 의 토근분류 기준을 활용 \n",
    "import nltk\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 12,\n",
       " 'is the': 2,\n",
       " 'the first': 7,\n",
       " 'first document': 1,\n",
       " 'the second': 9,\n",
       " 'second second': 6,\n",
       " 'second document': 5,\n",
       " 'and the': 0,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'is this': 3,\n",
       " 'this the': 13,\n",
       " 'the last': 8,\n",
       " 'last document': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram_range=(2, 2)\n",
    "vect = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **빈도계산**\n",
    "1. **max_df, min_df**를 사용하여 토큰횟수로 단어장을 구성\n",
    "1. 토큰의 빈도가 **max_df <= 측정빈도 <= min_df**를 대상\n",
    "1. 인수 값은 정수인 경우 **횟수**, 부동소수점인 경우 **비중**을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 3, 'is': 2, 'first': 1, 'document': 0},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 매칭 텍스트를 출력\n",
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스별 빈도값을 출력\n",
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**\n",
    "1. 모든문서에 존재하는 공통적단어의 가중치를 축소하여 재조정한다\n",
    "1. 구제적으로는 문서  dd (document)와 단어  tt  에 대해 다음과 같이 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hashing Trick**\n",
    "해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "\n",
    "# 전체 11,314개의 문서를 포함\n",
    "print(len(twenty.data)) \n",
    "twenty.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.66 s, sys: 48.8 ms, total: 5.71 s\n",
      "Wall time: 5.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해시함수를 사용하면 해시객체를 활용하여 속도가 빨라진다\n",
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 24 ms, total: 2.29 s\n",
      "Wall time: 2.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x300000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1786336 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=300000)\n",
    "\n",
    "%time hv.transform(twenty.data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example**\n",
    "실습예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [w            for w in hannanum.nouns(\" \".join(cell)) \n",
    "                     if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('컨테이너', 81),\n",
      " ('도커', 41),\n",
      " ('명령', 34),\n",
      " ('이미지', 33),\n",
      " ('사용', 26),\n",
      " ('가동', 14),\n",
      " ('중지', 13),\n",
      " ('mingw64', 13),\n",
      " ('삭제', 12),\n",
      " ('이름', 11)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(feature_name, count))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **텍스트 문장간의 유사도 측정**\n",
    "text 비교를 위한 tf-idf 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 문장간의 유사도 측정\n",
    "mydoclist = ['find what you love', \n",
    "             'do what you love', \n",
    "             \"don't do what you hate\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer   = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix       = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "document_distances = (tfidf_matrix * tfidf_matrix.T)\n",
    "print ('문장의 유사도 분석을 위한 {} X {} matrix를 만들었습니다.\\n {}'.format(\n",
    "    document_distances.get_shape()[0], \n",
    "    document_distances.get_shape()[1], \n",
    "    document_distances.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 문서간의 유사도 측정\n",
    "mydoclist = ['영희가 좋아하는 사람은 철수다.',\n",
    "             '철수를 영희가 좋아한다.',\n",
    "             '영희는 철수를 좋아하고 있다.']\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer   = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix       = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "document_distances = (tfidf_matrix * tfidf_matrix.T)\n",
    "print ('문장의 유사도 분석을 위한 {} X {} matrix를 만들었습니다.\\n {}'.format(\n",
    "       document_distances.get_shape()[0],\n",
    "       document_distances.get_shape()[1],\n",
    "       document_distances.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"https://t1.daumcdn.net/cfile/tistory/99B504445A3136B315\" align='left'></p>\n",
    "<br></br>\n",
    "\n",
    "1. **문장간 유사도가 20%를** 약간 넘고, 심지어 **전혀 유사도를 찾지 못하는 관계**도 존재한다\n",
    "1. 이를 통해보건데 **조사**등 일치하는 Token이 없으면 유사도를 찾지 못하는 결과로 예측이 된다\n",
    "1. 문장의 어근을 추출하여 한번더 위의 시험을 반복해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **조사등을 제외한 명사 Token을 생성한 뒤 유사도 측정**\n",
    "text 비교를 위한 tf-idf 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 : ['영희', '와', '철수', '는', '백구', '를', '산책', '시키', '기', '위하', '어', '한강', '에', '가', '었', '다', '.', '한강', '에', '도착', '하', '여', '누렁이', '를', '만나', '었', '다', '.']\n",
      "명사   : ['영희', '철수', '백구', '산책', '한강', '도착', '누렁이']\n",
      "품사   :[('영희', 'NNP'), ('와', 'JKM'), ('철수', 'NNG'), ('는', 'JX'), ('백구', 'NNG'), ('를', 'JKO'), ('산책', 'NNG'), ('시키', 'XSV'), ('기', 'ETN'), ('위하', 'VV'), ('어', 'ECS'), ('한강', 'NNP'), ('에', 'JKM'), ('가', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF'), ('한강', 'NNP'), ('에', 'JKM'), ('도착', 'NNG'), ('하', 'XSV'), ('여', 'ECS'), ('누렁이', 'NNG'), ('를', 'JKO'), ('만나', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "# 꼬꼬마 형태소 분석기\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    " \n",
    "kkma = Kkma() \n",
    "sentence = '영희와 철수는 백구를 산책시키기 위해 한강에 갔다. 한강에 도착하여 누렁이를 만났다.'\n",
    "print('형태소 : {}\\n명사   : {}\\n품사   :{}'.format(\n",
    "    kkma.morphs(sentence), # 형태소 구분 Token\n",
    "    kkma.nouns(sentence),  # 명사품사 Token\n",
    "    kkma.pos(sentence)))   # 형태소와 해당품사를 함께출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트위터 형태소 분석기\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.utils import pprint \n",
    "twitter  = Twitter()\n",
    "sentence = '영희와 철수는 백구를 산책시키기 위해 한강에 갔다. 한강에 도착하여 누렁이를 만났다.'\n",
    "print('형태소 : {}\\n명사   : {}\\n품사   :{}'.format(\n",
    "    twitter.morphs(sentence),\n",
    "    twitter.nouns(sentence),\n",
    "    twitter.pos(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoclist = ['영희가 사랑하는 강아지 백구를 산책시키고 있다.',\n",
    "             '철수가 사랑하는 소 누렁이를 운동시키고 있다.',\n",
    "             '영희와 철수는 소와 강아지를 산책 및 운동시키고 있다.']\n",
    "\n",
    "from konlpy.tag import Kkma, Twitter\n",
    "from konlpy.utils import pprint\n",
    "twitter = Twitter()\n",
    "kkma = Kkma()\n",
    "print('꼬꼬마 명사 : {}\\n트위터 명사 : {}'.format(\n",
    "    kkma.nouns(mydoclist[0]),\n",
    "    twitter.nouns(mydoclist[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoclist = ['영희가 사랑하는 강아지 백구를 산책시키고 있다.',\n",
    "             '철수가 사랑하는 소 누렁이를 운동시키고 있다.',\n",
    "             '영희와 철수는 소와 강아지를 산책 및 운동시키고 있다.']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Kkma, Twitter\n",
    "from konlpy.utils import pprint\n",
    "kkma = Kkma()\n",
    "doc_nouns_list = [] \n",
    "for doc in mydoclist:\n",
    "    nouns = kkma.nouns(doc)\n",
    "    doc_nouns_list.append(' '.join(nouns))\n",
    "    \n",
    "for i in range(0, 3):\n",
    "    print('doc' + str(i + 1) + ' : ' + str(doc_nouns_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer   = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix       = tfidf_vectorizer.fit_transform(doc_nouns_list) \n",
    "document_distances = (tfidf_matrix * tfidf_matrix.T)\n",
    "print ('문장의 유사도 분석을 위한 {} X {} matrix를 만들었습니다.\\n {}'.format(\n",
    "       document_distances.get_shape()[0],\n",
    "       document_distances.get_shape()[1],\n",
    "       document_distances.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
