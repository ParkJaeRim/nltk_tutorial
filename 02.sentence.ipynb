{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **Sentence 분석하기**\n",
    "자연어 문장의 분석방법 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "1. Text 자료를 파이썬으로 불러오기\n",
    "1. 불필요한 내용들 전처리\n",
    "1. Text 를 문장별/ 단어별 Token으로 변환하기\n",
    "1. 개별 Token의 속성값 (Tag) 을 부여하기\n",
    "1. Tag로 Token을 구분하여, 개별 Token을 활용하여 다양한 작업을 진행한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신조어, 사용자 임의의 Tag를 부여하고 싶은경우\n",
    "1. Tag 속성의 부여시, 문장 전체의 구조를 바탕으로 전개되지 않는다\n",
    "1. 개별 Token을, 모듈별 { dict } 목록을 활용하여 개별적으로 속성값을 부여한다\n",
    "1. 신조어, 사용자 임의 Tag를 부여하고 싶은 경우는\n",
    "    1. 전처리 작업에서 내용을 진행하고, 나머지를 진행한다 (진행한 단어들은 '힣힣힣' 구분 가능한 단어로 바꾼뒤, 마지막에 교체한다)\n",
    "    1. 태그작업이 완료된 뒤, 사용자 목록에 해당되는 단어들을 추출하여 사용자 태그로 내용을 변경한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **Phrase 분석**\n",
    "\n",
    "<br>\n",
    "## **1 구분분석**\n",
    "문장의 **단어별 Token** 속성을 활용하여 **구문 (phrase)** 추출하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('민병삼', 'Noun'), ('대령', 'Noun'), ('의', 'Josa'), ('항', 'Noun'), ('명', 'Suffix'), ('행위', 'Noun'), ('로', 'Josa'), ('초치', 'Noun'), ('하다', 'Verb')]\n",
      "CPU times: user 3.36 s, sys: 109 ms, total: 3.47 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 앞부분 Review\n",
    "text = '민병삼 대령의 항명행위로 초치했다'\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "words = twitter.pos(text, stem=True)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.RegexpParser with 3 stages>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "# 각 Tag 들을 중간에 묶을 Parsing 중간 객체를 정의한다\n",
    "# 중간객체 정의는 정규식을 활용한다\n",
    "grammar = \"\"\"\n",
    "NP: {<N.*>*<Suffix>?}   # 명사구를 정의한다\n",
    "VP: {<V.*>*}            # 동사구를 정의한다\n",
    "AP: {<A.*>*}            # 형용사구를 정의한다 \"\"\"\n",
    "parser = RegexpParser(grammar)\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [('민병삼', 'Noun'), ('대령', 'Noun')]), ('의', 'Josa'), Tree('NP', [('항', 'Noun'), ('명', 'Suffix')]), Tree('NP', [('행위', 'Noun')]), ('로', 'Josa'), Tree('NP', [('초치', 'Noun')]), Tree('VP', [('하다', 'Verb')])])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구문 파싱객체를 사용하여 Tree 구조를 시각화 하기\n",
    "chunks = parser.parse(words)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('민병삼', 'Noun'), ('대령', 'Noun')],\n",
       " [('항', 'Noun'), ('명', 'Suffix')],\n",
       " [('행위', 'Noun')],\n",
       " [('초치', 'Noun')],\n",
       " [('하다', 'Verb')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 생성한 Tree 구조를 List 객체로 묶어서 출력한다\n",
    "text_tree = [list(txt)    for txt in chunks.subtrees()]\n",
    "text_tree[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### **자연어 구문 분석과정**\n",
    "<br>\n",
    "1. 우선 언어학을 근간으로 한다\n",
    "1. <strike>말소리를 연구하는 **음운론(Phonology)** : 음성인식</strike>\n",
    "1. <strike>단어와 형태소를 연구하는 **형태론(Morphology)** : 형태소 분석</strike>\n",
    "1. 문법과 맥락/담화를 각각 논의하는 **통사론(syntax)** : 문법적 구조분석(Passing)\n",
    "1. 단어간의 의미차이를 구분하는 **의미론(Senmantics)** : 뉘앙스, 톤, 말하고자 하는 의도(긍/부정)\n",
    "1. 그리고 한글과 영어의 문화적 차이\n",
    "<br>\n",
    "<img src=\"http://i.imgur.com/1bhgstG.png\" align=\"left\" width=\"650\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활용방법\n",
    "# 문장의 구조도를 추출 가능하다\n",
    "# 1. 개별 단어의 Tag로 분석하기 어려운 구분을 분석한다\n",
    "# 1. 문장의 Ton을 세부적 분석한다 \n",
    "# 1. 긍/ 부정의 분석 등의 세부적인 내용을 분석한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 영어 같은 경우 (nltk) 문법 규칙들이 많이 성립되어 있어다\n",
    "1. CFG (Context Free Grammer) : '노암촘스키'가 만든 문법기준\n",
    "2. ATIS 문법 : 공항 안내시스템 개발용 문법규칙\n",
    "3. CKY 차트 파싱 알고리즘 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 한글의 경우\n",
    "1. Konlp, Konlpy\n",
    "1. 국립국어원 언어정보 나눔터\n",
    "1. https://ithub.korean.go.kr/user/total/database/corpusManager.do\n",
    "1. 영어 관련된 이론들을 정리해서 한글레 적용해 나간다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **2 단어를 활용한 문장분석**\n",
    "Word Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## **1 단어의 출현 빈도를 활용**\n",
    "word Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트에 지프의 법칙 적용\n",
    "\n",
    "    지프의 법칙(Zipf's law)에 따르면 텍스트의 토큰중 가장 많이 나오는 토큰과 가장 적게 나오는 토큰은 정비례한다.\n",
    "\n",
    "    지프의 법칙 : 어떠한 자연어 말뭉치 표현에 나타나는 단어들을 그 사용 빈도가 높은 순서대로 나열하였을 때, 모든 단어의 사용 빈도는 해당 단어의 순위에 반비례한다. 따라서 가장 사용 빈도가 높은 단어는 두 번째 단어보다 빈도가 약 두 배 높으며, 세 번째 단어보다는 빈도가 세 배 높다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'존경하는 독일 국민 여러분,\\n\\n고국에 계신 국민 여러분,\\n\\n하울젠 쾨르버재단 이사님과 모드로 전 동독 총리님을 비롯한 내외 귀빈 여러분,\\n\\n\\n먼저, 냉전과 분단을 넘어 통일을 이루'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 독일 퀘르버 재단 연설문 : 베를린 선언\n",
    "# 문서 불러오기\n",
    "f     = open('./data/베를린선언.txt', 'r')\n",
    "texts_Berlin_raw = f.read()\n",
    "f.close()\n",
    "texts_Berlin_raw[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', ',', '고국에', '계신', '국민', '여러분', ',']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Token 생성\n",
    "from nltk import FreqDist, word_tokenize\n",
    "texts = word_tokenize(texts_Berlin_raw)\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n"
     ]
    }
   ],
   "source": [
    "# Stop Word 전처리\n",
    "import string\n",
    "punct = string.punctuation\n",
    "punct = [punct[i] for i in range(len(punct))]\n",
    "punct = punct + ['\\n', ')', '(']\n",
    "\n",
    "# 불필요한 기호들을 제거한다\n",
    "for dump in punct: \n",
    "    texts = [txt.replace(dump, '')    for txt in texts]\n",
    "texts_Berlin = [txt    for txt in texts  if len(txt) > 2]\n",
    "print(len(texts_Berlin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "것입니다      28\n",
       "한반도       20\n",
       "있습니다      16\n",
       "합니다       16\n",
       "북한의       12\n",
       "한반도의      11\n",
       "북한이       11\n",
       "여러분       10\n",
       "평화를       10\n",
       "일입니다       6\n",
       "세계의        6\n",
       "남북이        6\n",
       "평화와        6\n",
       "국제사회의      6\n",
       "나가겠습니다     5\n",
       "군사적        5\n",
       "통일을        5\n",
       "새로운        5\n",
       "없습니다       4\n",
       "평화로운       4\n",
       "정부의        4\n",
       "평화의        4\n",
       "협력을        4\n",
       "이산가족       4\n",
       "도발을        4\n",
       "우리는        4\n",
       "김대중        4\n",
       "비핵화를       4\n",
       "나가야        4\n",
       "안전을        4\n",
       "          ..\n",
       "체결을        1\n",
       "평화협정       1\n",
       "부산과        1\n",
       "출발한        1\n",
       "화합을        1\n",
       "열차가        1\n",
       "치유하고       1\n",
       "구성원의       1\n",
       "일관성을       1\n",
       "분리해        1\n",
       "상황과        1\n",
       "정치‧군사적     1\n",
       "비정치적       1\n",
       "다섯째        1\n",
       "경제모델을      1\n",
       "실천하기만      1\n",
       "정상선언을      1\n",
       "공동번영할      1\n",
       "교량국가로      1\n",
       "해양을        1\n",
       "대륙과        1\n",
       "추진될        1\n",
       "협력사업들도     1\n",
       "가스관        1\n",
       "남·북·러      1\n",
       "유럽으로       1\n",
       "러시아와       1\n",
       "북경으로       1\n",
       "평양과        1\n",
       "인위적인       1\n",
       "Length: 923, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_token_dict = dict(FreqDist(texts_Berlin))\n",
    "\n",
    "import pandas as pd\n",
    "texts_token_series = pd.Series(texts_token_dict)\n",
    "texts_token_series.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## **2 불용어 처리**\n",
    "1. 직접적인 연관성이 낮은 단어들을 제외하고 분석을 한다\n",
    "1. 문장이 짧고, 함축적일 수록 불용어를 제외하면 성능이 낮은 경우가 많아서 특정한 규칙이 정해져 있지는 않다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like such a wonderful snow ice cream'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어의 경우\n",
    "# from nltk.corpus import stopwords\n",
    "texts = 'I like such a Wonderful Snow Ice Cream'\n",
    "texts = texts.lower()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'such', 'a', 'wonderful', 'snow', 'ice', 'cream']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(texts)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'her', 'those', 'an', 'into', 'further', 'such', 'now', 'mightn']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 목록을 추출\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[::18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'wonderful', 'snow', 'ice', 'cream']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word   for word in tokens   \n",
    "                 if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'이': 'NP',\n",
       " '있': 'VA',\n",
       " '하': 'VV',\n",
       " '것': 'NNB',\n",
       " '들': 'VV',\n",
       " '그': 'MM',\n",
       " '되': 'VV',\n",
       " '수': 'NNB',\n",
       " '보': 'VX',\n",
       " '않': 'VX',\n",
       " '없': 'VA',\n",
       " '나': 'VX',\n",
       " '사람': 'NNG',\n",
       " '주': 'VV',\n",
       " '아니': 'VCN',\n",
       " '등': 'NNB',\n",
       " '같': 'VA',\n",
       " '우리': 'NP',\n",
       " '때': 'NNG',\n",
       " '년': 'NNB',\n",
       " '가': 'VV',\n",
       " '한': 'MM',\n",
       " '지': 'VX',\n",
       " '대하': 'VV',\n",
       " '오': 'VV',\n",
       " '말': 'VX',\n",
       " '일': 'NNB',\n",
       " '그렇': 'VA',\n",
       " '위하': 'VV',\n",
       " '때문': 'NNB',\n",
       " '그것': 'NP',\n",
       " '두': 'VV',\n",
       " '말하': 'VV',\n",
       " '알': 'VV',\n",
       " '그러나': 'MAJ',\n",
       " '받': 'VV',\n",
       " '못하': 'VX',\n",
       " '그런': 'MM',\n",
       " '또': 'MAG',\n",
       " '문제': 'NNG',\n",
       " '더': 'MAG',\n",
       " '사회': 'NNG',\n",
       " '많': 'VA',\n",
       " '그리고': 'MAJ',\n",
       " '좋': 'VA',\n",
       " '크': 'VA',\n",
       " '따르': 'VV',\n",
       " '중': 'NNB',\n",
       " '나오': 'VV',\n",
       " '가지': 'VV',\n",
       " '씨': 'NNB',\n",
       " '시키': 'XSV',\n",
       " '만들': 'VV',\n",
       " '지금': 'NNG',\n",
       " '생각하': 'VV',\n",
       " '그러': 'VV',\n",
       " '속': 'NNG',\n",
       " '하나': 'NR',\n",
       " '집': 'NNG',\n",
       " '살': 'VV',\n",
       " '모르': 'VV',\n",
       " '적': 'XSN',\n",
       " '월': 'NNB',\n",
       " '데': 'NNB',\n",
       " '자신': 'NNG',\n",
       " '안': 'MAG',\n",
       " '어떤': 'MM',\n",
       " '내': 'VV',\n",
       " '경우': 'NNG',\n",
       " '명': 'NNB',\n",
       " '생각': 'NNG',\n",
       " '시간': 'NNG',\n",
       " '그녀': 'NP',\n",
       " '다시': 'MAG',\n",
       " '이런': 'MM',\n",
       " '앞': 'NNG',\n",
       " '보이': 'VV',\n",
       " '번': 'NNB',\n",
       " '다른': 'MM',\n",
       " '어떻': 'VA',\n",
       " '여자': 'NNG',\n",
       " '개': 'NNB',\n",
       " '전': 'NNG',\n",
       " '사실': 'NNG',\n",
       " '이렇': 'VA',\n",
       " '점': 'NNG',\n",
       " '싶': 'VX',\n",
       " '정도': 'NNG',\n",
       " '좀': 'MAG',\n",
       " '원': 'NNB',\n",
       " '잘': 'MAG',\n",
       " '통하': 'VV',\n",
       " '소리': 'NNG',\n",
       " '놓': 'VX'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글의 경우 별도로 패키지가 없다\n",
    "# 같은 한글단어라도 품사에 따라 다르게 작용하므로 품사별 단어를 함께 정리\n",
    "# 분석 대상 및 성격에 맞는 불용어 사전을 알맞게 조정해서 활용할 필요가 있다\n",
    "# 쉽지 않아.. 쉽지 않아...\n",
    "f = open('./data/한국어불용어100.txt', 'r')\n",
    "s = f.read(); f.close()\n",
    "\n",
    "stop_words = [ txt.split('\\t')[:2]  for txt in s.split('\\n') ]\n",
    "stopword   = {}\n",
    "for txt in stop_words:\n",
    "    try:    stopword[txt[0]] = txt[1]\n",
    "    except: pass\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **3-1 통계적 이론의 활용**\n",
    "문장의 Token을 [ list ] 객체를 활용한 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **레벤슈타인 편집거리 는**  [참고슬라이드](https://www.slideshare.net/eungihong16/ss-56515076)\n",
    "1. 철자의 오류수정, 비슷한 어구 검색 등으로 활용되고\n",
    "1. 의학분야에서는 DNA 배열의 유사성을 판단하는 기준으로 사용한다\n",
    "1. 1번 작업하는 위치는 중요하지 않지만 작업 순번에 따라 최소한의 작업내용을 요약 가능해야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원본 Text\n",
    "text1 = \"자연 언어에 대한 연구는 오래전부터 이어져 오고 있음에도 2018년까지도 사람처럼 이해하지는 못한다.\".split()\n",
    "# 몇가지 단어를 바꾼다\n",
    "text2 = \"자연 언어에 대한 연구는 오래전부터 이어져 들어서도 아직 컴퓨터가 사람처럼 이해하지는 못한다.\".split()\n",
    "# text2 와 동일한 단어의 어순만 변경\n",
    "text3 = \"자연 아직 컴퓨터가 언어에 들어서도 못한다 이어져 사람처럼 이해하지는 대한 연구는 오래전부터.\".split()\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 01 편집거리의 계산\n",
    "# 두 단어/ 문장이 같은 내용이 되려면 몇번의 수정을 필요로 하는지 계산\n",
    "from nltk.metrics import edit_distance\n",
    "edit_distance('파이썬 알고리즘', '파파미 알탕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 : 3 \n",
      "어휘 순서를 바꿨을 때 : 10\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 : {} \\n어휘 순서를 바꿨을 때 : {}'.format(\n",
    "    edit_distance(text1, text2), \n",
    "    edit_distance(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 02 accuracy 정확도 측정\n",
    "from nltk.metrics import accuracy\n",
    "accuracy('파이썬', '파이프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.08333\n"
     ]
    }
   ],
   "source": [
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    accuracy(text1, text2), \n",
    "    accuracy(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## **3-2 통계적 이론의 활용**\n",
    "문장의 Token을 { set } 객체를 활용한 분석\n",
    "1. precision = Correct / (Correct + Incorrect + Missing)\n",
    "1. recall = Correct / (Correct + Incorrect + Spurious<가짜>)\n",
    "1. f_measure = (2 X Precision X Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원본 Text\n",
    "text1 = set(text1)\n",
    "text2 = set(text2)\n",
    "text3 = set(text3)\n",
    "len(text1), len(text2), len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import precision\n",
    "precision({'파이썬'}, {'파르썬'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "# precision 정확도 측정\n",
    "# List 객체가 아닌 Set 객체를 활용하므로 어순을 무시한 유사도 측정을 한다\n",
    "\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    precision(set(text1), set(text2)), \n",
    "    precision(set(text2), set(text3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "# recall 어휘의 재현율\n",
    "\n",
    "from nltk.metrics import recall\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    recall(text1, text2), \n",
    "    recall(text2, text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생략된 단어가 다를 때 0.75 \n",
      "어휘 순서를 바꿨을 때 0.8333\n"
     ]
    }
   ],
   "source": [
    "# f-measure 측정\n",
    "\n",
    "from nltk.metrics import f_measure\n",
    "print('생략된 단어가 다를 때 {:.4} \\n어휘 순서를 바꿨을 때 {:.4}'.format(\n",
    "    f_measure(text1, text2), \n",
    "    f_measure(text2, text3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **3 N-gram 의 활용**\n",
    "1. **고전적인 통계적 이론** 및 , **MLP(Multi Line Perceptron) 알고리즘 분석**시 Token의 갯수가 일치해야 한다\n",
    "1. 정해진 기준보다 갯수가 적으면 **Zero Padding**을 활용하여 전체 길이는 맞춘다\n",
    "1. Text 분석량이 많아질수록 불필요한 저장공간과, 전체 Token을 별도의 차원으로 나눠야 하는 등의 효율성 문제가 발생 \n",
    "1. 따라서 문장별 길이의 무관하게, **일정한 기준에 따른 단위별 묶음** 을 활용함으로써 다양한 추가적 분석기법을 활용 가능하다\n",
    "1. 단점으로는 N-gram을 활용함에 따라 원본 데이터가 늘어난다\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/8ARA1.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## **1 N-gram 생성하기**\n",
    "Token 을 생성한 뒤, 갯수별로 Tuple을 묶는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는',\n",
       " '여러분',\n",
       " '고국에',\n",
       " '여러분',\n",
       " '하울젠',\n",
       " '쾨르버재단',\n",
       " '이사님과',\n",
       " '모드로',\n",
       " '총리님을',\n",
       " '비롯한',\n",
       " '여러분',\n",
       " '냉전과',\n",
       " '분단을',\n",
       " '통일을',\n",
       " '이루고',\n",
       " '힘으로',\n",
       " '유럽통합과',\n",
       " '국제평화를',\n",
       " '선도하고',\n",
       " '독일과']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_sample = texts_Berlin[:20]\n",
    "texts_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('존경하는', '여러분', '고국에'),\n",
       " ('여러분', '고국에', '여러분'),\n",
       " ('고국에', '여러분', '하울젠'),\n",
       " ('여러분', '하울젠', '쾨르버재단'),\n",
       " ('하울젠', '쾨르버재단', '이사님과')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영문의 경우 띄어쓰기를 기준으로 Token을 생성한다\n",
    "# 한글의 경우 같은 단어여도, 단어의 위치 조사와 결합여부에 따라 개별 단어 Token 간의 위상값이 달라진다\n",
    "# 이번의 경우에는 Twitter 모듈을 활용하여 Tag를 정의한다\n",
    "from nltk.util import ngrams\n",
    "\n",
    "texts_sample = [txt for txt in ngrams(texts_sample, 3)]\n",
    "texts_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## **2 Point wise Mutual Information**\n",
    "1. PMI : https://www.slideshare.net/RetrieverJo/pmi-twitter-57723391\n",
    "1. 단어간의 거리를 비교측정하여 상관성을 분석한다\n",
    "1. Bi-gram, Tri-gram을 기준을 두고서 문장을 분석한다\n",
    "1. 연어 (근접어:collocation) 관계를 통해서 문장성분을 분석가능한 객체를 생성한다\n",
    "1. PMI 는 단어간의 상관관계 확률론을 근거로, **단어간의 독립을 가정할 때 발생확률**과 **문서에서 측정된 동시발생확률**을 비교하여 상관성을 분석한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국에', '계신', '국민', '여러분', '하울젠', '쾨르버재단']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text 내용 중 한글만 추출한다\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "re_capt = RegexpTokenizer('[가-힣]\\w+')\n",
    "raw_texts = re_capt.tokenize(texts_Berlin_raw)\n",
    "raw_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'존경하는 독일 국민 여러분 고국에 계신 국민 여러분 하울젠 쾨르버재단 이사님과 모드로 동독 총리님을 비롯한 내외 귀빈 여러분 먼저 냉전과 분단을 넘어 통일을 이루고 힘으로 유럽통합과 국제평화를 선도하고 있는 독일과 독일 국민에게 무한한 경의를 표합니다 오늘 자리를 마련해 주신 독일 정부와 쾨르버 재단에도 감사드립니다 아울러 얼마 별세하신 헬무트 총리의 가족'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ''\n",
    "for txt in raw_texts:\n",
    "    texts += txt + \" \"\n",
    "texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 51.4 ms, total: 1.83 s\n",
      "Wall time: 565 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 베를린 선언문에 Tag 속성 추가하기\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "tagged_words = twitter.pos(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### **01 Bi-Gram 을 대상**\n",
    "어휘간 우도 추출\n",
    "http://konlpy-ko.readthedocs.io/ko/v0.4.3/examples/collocations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7fc643f2ddd8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bi-gram 을 대상으로 단어간 최대우도를 갖는 관계 측정\n",
    "from nltk import collocations\n",
    "\n",
    "finder = collocations.BigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun')),\n",
       " (('감사', 'Noun'), ('드립니', 'Verb')),\n",
       " (('갖춰', 'Verb'), ('지', 'PreEomi')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('거나', 'Eomi'), ('깨져', 'Verb')),\n",
       " (('건너지', 'Verb'), ('않기', 'Verb')),\n",
       " (('걷어', 'Verb'), ('차는', 'Verb')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 n-grams with highest PMI\n",
    "measures = collocations.BigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### **02 Tri-Gram 을 대상**\n",
    "어휘간 우도 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.TrigramCollocationFinder at 0x7fc643f2db70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tri-gram 을 대상으로 단어간 최대우도를 갖는 관계 측정\n",
    "finder = collocations.TrigramCollocationFinder.from_words(tagged_words)\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가능하며', 'Adjective'), ('불가', 'Noun'), ('역적', 'Noun')),\n",
       " (('가스', 'Noun'), ('관', 'Noun'), ('연결', 'Noun')),\n",
       " (('가운데', 'Noun'), ('현재', 'Noun'), ('생존', 'Noun')),\n",
       " (('같은', 'Adjective'), ('공감', 'Noun'), ('대', 'Suffix')),\n",
       " (('거나', 'Eomi'), ('깨져', 'Verb'), ('서도', 'Noun')),\n",
       " (('검증', 'Noun'), ('가능하며', 'Adjective'), ('불가', 'Noun')),\n",
       " (('견', 'Noun'), ('지하', 'Noun'), ('면서', 'Noun')),\n",
       " (('과도', 'Josa'), ('같은', 'Adjective'), ('공감', 'Noun')),\n",
       " (('들어서는', 'Verb'), ('대전', 'Noun'), ('환', 'Noun')),\n",
       " (('록', 'Eomi'), ('앞장서서', 'Verb'), ('돕겠', 'Verb'))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 n-grams with highest PMI\n",
    "measures = collocations.TrigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **3 은닉 마르코프 모델 추정**\n",
    "http://untitledtblog.tistory.com/31 (알고리즘해설)\n",
    "Hidden Markov Model estimation\n",
    "\n",
    "1. $x_0, x_1..$ : 은닉상태 , $y_0, y_1 ..$ : 관찰 가능한 상태\n",
    "1. 명확한 확률분포와 관련된 상태의 유한집합으로 구성된 분석방법이다\n",
    "1. 단점으로는 많은 훈련을 필요로 하고, 큰 의존성을 사용될 수 없다\n",
    "\n",
    "<p><img src=\"http://iacs-courses.seas.harvard.edu/courses/am207/blog/hmm.png\" align='left' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### **01 Hidden Markov Model estimation**\n",
    "HMM추정을 사용해 테스트를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는 독일 국민 여러분,',\n",
       " '고국에 계신 국민 여러분,',\n",
       " '하울젠 쾨르버재단 이사님과 모드로 전 동독 총리님을 비롯한 내외 귀빈 여러분,',\n",
       " '\\n먼저, 냉전과 분단을 넘어 통일을 이루고,',\n",
       " '그 힘으로 유럽통합과 국제평화를 선도하고 있는',\n",
       " '독일과 독일 국민에게 무한한 경의를 표합니다.',\n",
       " '\\n오늘 이 자리를 마련해 주신',\n",
       " '독일 정부와 쾨르버 재단에도 감사드립니다.',\n",
       " '\\n아울러, 얼마 전 별세하신 故 헬무트 콜 총리의 가족과 ',\n",
       " '독일 국민들에게 깊 은 애도와 위로의 마음을 전합니다.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = texts_Berlin_raw.split('\\n\\n')\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는 독일 국민 여러분',\n",
       " '고국에 계신 국민 여러분',\n",
       " '하울젠 쾨르버재단 이사님과 모드로 전 동독 총리님을 비롯한 내외 귀빈 여러분',\n",
       " '먼저 냉전과 분단을 넘어 통일을 이루고',\n",
       " '그 힘으로 유럽통합과 국제평화를 선도하고 있는',\n",
       " '독일과 독일 국민에게 무한한 경의를 표합니다',\n",
       " '오늘 이 자리를 마련해 주신',\n",
       " '독일 정부와 쾨르버 재단에도 감사드립니다',\n",
       " '아울러 얼마 전 별세하신 故 헬무트 콜 총리의 가족과 ',\n",
       " '독일 국민들에게 깊 은 애도와 위로의 마음을 전합니다']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [text.replace('\\n', '')  for text in texts]\n",
    "texts = [text.replace(',', '')  for text in texts]\n",
    "texts = [text.replace('.', '')  for text in texts]\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('존경하는', 'Verb'), ('독일', 'Noun'), ('국민', 'Noun'), ('여러분', 'Noun')],\n",
       " [('고국', 'Noun'),\n",
       "  ('에', 'Josa'),\n",
       "  ('계신', 'Verb'),\n",
       "  ('국민', 'Noun'),\n",
       "  ('여러분', 'Noun')]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장별 List로 구분하여 tag 목록을 생성하기\n",
    "tagged_words = [twitter.pos(text)  for text in texts]\n",
    "tagged_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Verb',\n",
       " 'Noun',\n",
       " 'Josa',\n",
       " 'Suffix',\n",
       " 'Adjective',\n",
       " 'Eomi',\n",
       " 'Exclamation',\n",
       " 'Foreign',\n",
       " 'Determiner',\n",
       " 'Number',\n",
       " 'Punctuation',\n",
       " 'Alpha',\n",
       " 'PreEomi',\n",
       " 'Adverb',\n",
       " 'Conjunction',\n",
       " 'VerbPrefix']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 분석에서 사용된 Tag 를 추출\n",
    "from nltk.util import unique_list\n",
    "\n",
    "tag_set = unique_list( tag     for sent        in  tagged_words     \n",
    "                               for (word, tag) in  sent )\n",
    "print(len(tag_set))\n",
    "tag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['존경하는', '독일', '국민', '여러분', '고국', '에', '계신', '하울', '젠', '쾨르버']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence 에 사용된 고유 단어들을 추출\n",
    "symbols = unique_list( word     for sent        in  tagged_words    \n",
    "                                for (word, tag) in  sent )\n",
    "print(len(symbols))\n",
    "symbols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.69%\n"
     ]
    }
   ],
   "source": [
    "# 히든 마르코프 훈련모듈 활성화 (92개 tag,  1464개 문장) \n",
    "def train_and_test(tagged_words, est):\n",
    "\n",
    "    from nltk.util import unique_list\n",
    "    # 문장에 사용된 중복된 tag 를 요약\n",
    "    tag_set = unique_list( tag   for sent        in  tagged_words     \n",
    "                                 for (word, tag) in  sent )\n",
    "    # 문장에 사용된 중복된 단어를 요약\n",
    "    symbols = unique_list( word  for sent        in  tagged_words    \n",
    "                                 for (word, tag) in  sent )\n",
    "    import nltk\n",
    "    trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "    train_corpus, test_corpus = [], []  \n",
    "    for i in range(len(tagged_words)) :\n",
    "        # train 90% , test 10% 데이터 생성\n",
    "        if i % 10 :  \n",
    "            train_corpus += [tagged_words[i]]\n",
    "        else :       \n",
    "            test_corpus += [tagged_words[i]]\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator = est)\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))\n",
    "\n",
    "train_and_test(tagged_words, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성분석의 경우 Text로 변경시 HMM을 활용 가능하고, 성능또한 우수하다\n",
    "# https://github.com/jihyun300/Speech-Recognizer\n",
    "# Tensorflow, Pytorch 등의 머신러닝 모듈을 사용할 때 보다 가볍고, 설정의 변경이 용이하다\n",
    "# 실제로 맞춤범 검사의 경우, 머신러닝/ 딥러닝 보다 \"퍼지이론\"을 활용한 전문적인 모듈이 우수한 성능을 나타낸다\n",
    "# Text 분석의 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영문의 경우 Brown 어휘목록을 활용한 HMM추정\n",
    "def HMM_english():\n",
    "    import nltk\n",
    "    cor = nltk.corpus.brown.tagged_sents(categories='adventure')[:500]\n",
    "\n",
    "    from nltk.util import unique_list\n",
    "    tag_set = unique_list( tag  for sent        in  cor     \n",
    "                                for (word, tag) in  sent )\n",
    "    symbols = unique_list( word  for sent        in  cor    \n",
    "                                 for (word, tag) in  sent )\n",
    "    print(\"tag set  : {} \\nWord set : {}\".format(len(tag_set), len(symbols)))\n",
    "\n",
    "    train_corpus, test_corpus = [], []  \n",
    "    trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "    for i in range(len(cor)) :\n",
    "        if i % 10 :  train_corpus += [cor[i]]   # train 90% , test 10% 데이터 생성\n",
    "        else :       test_corpus += [cor[i]]\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=None)  # .train_supervised()\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag set  : 92 \n",
      "Word set : 1464\n",
      "22.75%\n"
     ]
    }
   ],
   "source": [
    "# 히든 마르코프 훈련모듈 활성화 (92개 tag,  1464개 문장) \n",
    "HMM_english()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **4 TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency\n",
    "1. 문서의 내용을 쉽게 벡터로 표현하는 고전적인 방식\n",
    "1. Term Frequency (용어의 빈도) : 특정 용어의 발생빈도를 연산 (문서에서 Token의 출현빈도)/(문서의 전체 Token의 갯수)\n",
    "1. Inverse Document Frequency (문서 빈도의 역) : 해당 문서를 이해하는데 단어의 중요도 (일반 문서에서의 출현빈도 대비 해당 문서에서의 출현빈도)\n",
    "1. **TF**는 해당 문서가 있으면 바로 연산이 가능하지만\n",
    "1. 하지만 **IDF**는 모집단의 Corpus 별로 **일반적인 문서에서 개별 Token의 출현빈도**를 계산한 결과값을 기본으로 제공해야만 연산이 가능하다\n",
    "1. 한글의 경우 **Konlpy** 에서 제공하는 **Komoran** 을 활용하면 **IDF**를 연산 가능하다\n",
    "\n",
    "### Source  Code : https://stackoverflow.com/questions/45232671/obtain-tf-idf-weights-of-words-with-sklearn\n",
    "\n",
    "### Image : https://blog.exploratory.io/quantifying-documents-by-calculating-tf-idf-in-r-94a4ccea2b38\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1400/1*fooU_APWpC5HypRHYxeJGQ.png\" align=\"left\" width='500'><br>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*hno6Kll3MSBdp7udl3E1tw.png\" align=\"left\" width='500'><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief', 'Justice', 'Roberts', ',', 'President']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 연설문 파일 불러오기\n",
    "f = open('./data/trump.txt', 'r')\n",
    "texts_org = f.read()\n",
    "f.close()\n",
    "\n",
    "# Word Token으로 변환\n",
    "from nltk import word_tokenize\n",
    "texts = word_tokenize(texts_org)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation 문장기호 와 Stopword 목록 추출\n",
    "from nltk.corpus import stopwords\n",
    "stopword_eng = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "punct = string.punctuation\n",
    "punct = [punct[i] for i in range(len(punct))]\n",
    "punct = punct + stopword_eng + ['\\n'] \n",
    "len(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token 중에 punct 에 포함되지 않은 단어들로만 추출하여 목록을 새로 만들고\n",
    "# 새로 만든 token을 사용하여 Text 새롭게 재구성하기\n",
    "texts = [txt.lower()    for txt in texts   if txt.lower() not in punct]\n",
    "document = ''\n",
    "for txt in texts:\n",
    "    document += txt + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x456 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 456 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-Learn 을 사용하여 Tf-idf 연산\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec   = TfidfVectorizer()\n",
    "transformed = tfidf_vec.fit_transform(raw_documents = [document])\n",
    "index_value = {i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\n",
    "\n",
    "# tfidf 학습결과를 저장한 객체로 바로 내용을 알 수 없다\n",
    "transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf 연산결과를 담을 객체\n",
    "fully_indexed = []\n",
    "\n",
    "import numpy as np\n",
    "transformed = np.array(transformed.todense())\n",
    "\n",
    "for row in transformed:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in enumerate(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "america     0.423619\n",
       "american    0.232990\n",
       "people      0.211809\n",
       "country     0.190628\n",
       "nation      0.190628\n",
       "one         0.169447\n",
       "every       0.148266\n",
       "never       0.127086\n",
       "world       0.127086\n",
       "back        0.127086\n",
       "new         0.127086\n",
       "great       0.127086\n",
       "make        0.105905\n",
       "god         0.105905\n",
       "today       0.105905\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tfidf = pd.Series(fully_indexed[0]).sort_values(ascending=False)\n",
    "tfidf[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12708556268223822"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['great']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
